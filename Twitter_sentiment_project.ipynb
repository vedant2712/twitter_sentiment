{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f609e264",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ccfb003",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eefb3b94273c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# utilities\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#nlp\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649eb08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "\n",
    "with open('kaggle.json', 'w') as f:\n",
    "  f.write('{\"username\":\"yashgroot\",\"key\":\"56590d8c3eb21dc01ac54fc1c3f39663\"}')\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d kazanova/sentiment140\n",
    "!unzip sentiment140.zip -d ./\n",
    "# Importing the dataset\n",
    "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "dataset=pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = DATASET_ENCODING,  names = DATASET_COLUMNS)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55919f0b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['sentiment','text']]\n",
    "# dataset\n",
    "ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution on basis of sentiments', legend=True)\n",
    "ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
    "dataset['sentiment'] = dataset['sentiment'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3945289",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_negative = dataset.head(800000)\n",
    "dataset_positive = dataset.tail(800000)\n",
    "dataset_positive_sampled = dataset_positive.sample(n=20000)\n",
    "dataset_negative_sampled = dataset_negative.sample(n=20000)\n",
    "dataset_sampled = [dataset_negative_sampled,dataset_positive_sampled]\n",
    "dataset_sampled = pd.concat(dataset_sampled)\n",
    "tweets, sentiment = list(dataset_sampled['text']), list(dataset_sampled['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f76a50",
   "metadata": {},
   "source": [
    "# Distribution of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38298835",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_positive[\"word_count\"] = dataset_positive['text'].apply(lambda x: len(str(x).split()))\n",
    "sns.distplot(dataset_positive.word_count, kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_negative[\"word_count\"] = dataset_negative['text'].apply(lambda x: len(str(x).split()))\n",
    "sns.distplot(dataset_negative.word_count, kde=False, rug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da7353",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token in ['hahah', 'hahaha', 'hahahaha']:\n",
    "        return 'haha'\n",
    "    if token in ['lmao', 'lolz', 'rofl']:\n",
    "        return 'lol'\n",
    "    if token == 'goood':\n",
    "        return 'good'\n",
    "    if token == 'thanx' or token == 'thnx':\n",
    "        return 'thanks'\n",
    "    if token in [\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\", \"hadn't\",\"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"shouldn't\", \"ain't\", \"they've\"]:\n",
    "        return token.replace(\"'\", \"\")\n",
    "    if token == 'bday' or token == 'b-day':\n",
    "        return 'birthday'\n",
    "    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == 'Â½25' or token == 'URL':\n",
    "        return ''\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "pre_processed_text, cleaned_token_list = pre_process(tweets,sentiment)\n",
    "print(f'Text Preprocessing complete.')\n",
    "print(f'Time Taken: {round(time.time()-t)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d8d31",
   "metadata": {},
   "source": [
    "# Wordcount visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = pre_processed_text[:20000]\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bb86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = pre_processed_text[20000:]\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "              collocations=False).generate(\" \".join(data_pos))\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sampled['pre_processed_text'] = pre_processed_text\n",
    "dataset_sampled['cleaned'] = dataset_sampled.pre_processed_text.apply(nlp)\n",
    "dataset_sampled = dataset_sampled.replace({'sentiment': 0}, 'negative')\n",
    "dataset_sampled = dataset_sampled.replace({'sentiment': 1}, 'positive')\n",
    "dataset_sampled\n",
    "# dataset_sampled.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromParsedDocuments(dataset_sampled, category_col='sentiment', parsed_col='cleaned').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c7b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "                                       category='positive',\n",
    "                                       category_name='positive',\n",
    "                                       not_category_name='negative',\n",
    "                                       minimum_term_frequency=5,\n",
    "                                       width_in_pixels=1000,\n",
    "                                       transform=st.Scalers.percentile,\n",
    "                                       metadata=dataset_sampled['text'])\n",
    "file_name = 'ScattertextLog.html'\n",
    "with open(file_name, 'wb') as fp: \n",
    "    fp.write(html.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.HTML(filename='/content/ScattertextLog.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3bffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d95df4",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1  = time.time()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset_sampled = dataset_sampled.replace({'sentiment': 'negative'}, 0)\n",
    "dataset_sampled = dataset_sampled.replace({'sentiment': 'positive'}, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_sampled['pre_processed_text'],dataset_sampled['sentiment'] , test_size=0.30, random_state=42)\n",
    "print(X_train, X_test, y_train, y_test)\n",
    "# dataset_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,3)) \n",
    "vectorizer.fit(X_train)          \n",
    "x_tr=vectorizer.transform(X_train)\n",
    "x_te=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from itertools import chain\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model, X_test_data, Y_test_data):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred_cont = model.predict(X_test_data)\n",
    "    y_pred_bin = []\n",
    "    for i in y_pred_cont:\n",
    "      if 1- i <= i - 0:\n",
    "        y_pred_bin.append(1)\n",
    "      else : \n",
    "        y_pred_bin.append(0)\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(Y_test_data, y_pred_bin))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(Y_test_data, y_pred_bin)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b301ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()  \n",
    "parameters = {'alpha':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5,\n",
    "10, 50, 100]}\n",
    "clf = GridSearchCV(model, parameters, cv=10,scoring='roc_auc',return_train_score=True)\n",
    "clf.fit(x_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc= results['mean_train_score'].values  #extracting the auc scores \n",
    "cv_auc = results['mean_test_score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=[]\n",
    "for i in parameters.values():\n",
    "    a1.append(i)\n",
    "alphas = list(chain.from_iterable(a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, train_auc, label='Train AUC')\n",
    "plt.plot(alphas, cv_auc, label='CV AUC')\n",
    "plt.scatter(alphas, train_auc, label='Train AUC points')\n",
    "plt.scatter(alphas, cv_auc, label='CV AUC points')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Alpha: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Hyper parameter Vs AUC plot\")  \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee33f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparam=clf.best_params_['alpha']   #extracting the best hyperparameter\n",
    "print(\"The best Alpha=\",bestparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_model = MultinomialNB(alpha=bestparam) #Building a Naive Bayes model with the best alpha\n",
    "mul_model.fit(x_tr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a77e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = mul_model.predict_proba(x_tr)[:,1]  #Prediction using the model(log probability of each class)\n",
    "y_test_pred = mul_model.predict_proba(x_te)[:,1]\n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)   \n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.title(\"AUC PLOTS\")             #Plotting train and test AUC \n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trauc=round(auc(train_fpr, train_tpr),3)\n",
    "teauc=round(auc(test_fpr, test_tpr),3)\n",
    "print('Train AUC=',trauc)\n",
    "print('Test AUC=',teauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]      #finding the best threashold \n",
    "    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n",
    "    return t\n",
    "\n",
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:                                 #building a confusion matrix with the best threashold \n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "TRCM=confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t))\n",
    "TECM=confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t))\n",
    "\n",
    "def CM(x,y):\n",
    "    labels = ['TN','FP','FN','TP']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in x.flatten()]\n",
    "                    \n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "    zip(labels,group_counts)]\n",
    "    labels = np.asarray(labels).reshape(2,2)       #Building a design for the confusion matrix\n",
    "    sns.heatmap(x, annot=labels, fmt='', cmap='BuPu')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(y)\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b84c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM(TRCM,'Train Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM(TECM,'Test Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \",(TRCM[0,0]+TRCM[1,1])/np.sum(TRCM)*100)   \n",
    "print(\"Test accuracy = \",(TECM[0,0]+TECM[1,1])/np.sum(TECM)*100)\n",
    "print(f'Time Taken by Naive Bayes Model After Preprocessing: {round(time.time()-t1)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5186232",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8edb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(x_tr, y_train)\n",
    "model_Evaluate(LRmodel, x_te, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfbd6c",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c91063",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ffe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3627a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    dot = np.dot(u, v)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "unks = []\n",
    "UNKS = []\n",
    "\n",
    "# This function will act as a \"last resort\" in order to try and find the word\n",
    "# in the words embedding layer. It will basically eliminate contiguously occuring\n",
    "# instances of a similar character\n",
    "def cleared(word):\n",
    "    res = \"\"\n",
    "    prev = None\n",
    "    for char in word:\n",
    "        if char == prev: continue\n",
    "        prev = char\n",
    "        res += char\n",
    "    return res\n",
    "\n",
    "\n",
    "def sentence_to_indices(sentence_words, word_to_index, max_len, i):\n",
    "    global X, Y\n",
    "    sentence_indices = []\n",
    "    for j, w in enumerate(sentence_words):\n",
    "        try:\n",
    "            index = word_to_index[w]\n",
    "        except:\n",
    "            UNKS.append(w)\n",
    "            w = cleared(w)\n",
    "            try:\n",
    "                index = word_to_index[w]\n",
    "            except:\n",
    "                index = word_to_index['unk']\n",
    "                unks.append(w)\n",
    "        X[i, j] = index\n",
    "\n",
    "        \n",
    "# Here we will utilize the already computed 'cleaned_tokens_list' variable\n",
    "   \n",
    "print('Removed Noise, CPU Time:', time.time() - start_time)\n",
    "start_time = time.time()\n",
    "\n",
    "list_len = [len(i) for i, j in cleaned_token_list]\n",
    "max_len = max(list_len)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "X = np.zeros((len(cleaned_token_list), max_len))\n",
    "Y = np.zeros((len(cleaned_token_list), ))\n",
    "\n",
    "for i, tk_lb in enumerate(cleaned_token_list):\n",
    "    tokens, label = tk_lb\n",
    "    sentence_to_indices(tokens, word_to_index, max_len, i)\n",
    "    Y[i] = label\n",
    "    \n",
    "print('Data Prepared for model, CPU Time:', time.time() - start_time)\n",
    "\n",
    "\n",
    "print(X[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce480db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"unk\"].shape[0] #50\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5593a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = Sequential()\n",
    "\n",
    "LSTM_model.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\n",
    "LSTM_model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "LSTM_model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n",
    "LSTM_model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.fit(X_train_data, Y_train_data, validation_data=(X_test_data, Y_test_data), epochs = 20, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(history):\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(LSTM_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = word_to_index['unk']\n",
    "\n",
    "n_unk_words = 0\n",
    "\n",
    "for x in X:\n",
    "    for y in x:\n",
    "        if y == unk:\n",
    "            n_unk_words += 1\n",
    "\n",
    "n_unk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(unks).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model_clean_data = Sequential()\n",
    "\n",
    "model_clean_data.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\n",
    "model_clean_data.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "model_clean_data.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n",
    "model_clean_data.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model_clean_data.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clean_data.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data, X_test_data, Y_train_data, Y_test_data = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clean_data.fit(X_train_data, Y_train_data, validation_data=(X_test_data, Y_test_data), epochs = 10, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(model_clean_data.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Evaluate(model_clean_data, X_test_data, Y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a75b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
